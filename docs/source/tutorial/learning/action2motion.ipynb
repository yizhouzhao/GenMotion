{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Conditioned Generation of 3D Human Motions (Action2Motion)\n",
    "\n",
    "Action2Motion can be seen as an inverse of action recognition: given a prescribed action type, it aims to generate plausible human motion sequences in 3D. Importantly, the set of generated motions are expected to maintain its diversity to be able to explore the entire action-conditioned motion space; meanwhile, each sampled sequence faithfully resembles a natural human body articulation dynamics. Motivated by these objectives, Action2Motion follows the physics law of human kinematics by adopting the Lie Algebra theory to represent the natural human motions; we also propose a temporal Variational Auto-Encoder (VAE) that encourages a diverse sampling of the motion space. \n",
    "\n",
    "See more from [original implementation](https://ericguo5513.github.io/action-to-motion/), and [paper link](https://arxiv.org/pdf/2007.15240.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To get the pre-process the dataset, please refer to the this [Github repository](https://github.com/Mathux/ACTOR) and agree to the license. There following code shows examples from `HumanAct12` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data path\n",
    "data_path = \"E://researches/action-to-motion/dataset/humanact12\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genmotion.algorithm.action2motion.configs import params\n",
    "from genmotion.algorithm.action2motion.utils import paramUtil\n",
    "from genmotion.algorithm.action2motion.dataset import MotionFolderDatasetHumanAct12, MotionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arbitrary_len': False, 'batch_size': 8, 'checkpoints_dir': './checkpoints/vae', 'clip_set': './dataset/pose_clip_full.csv', 'coarse_grained': True, 'dataset_type': 'humanact12', 'decoder_hidden_layers': 2, 'dim_z': 30, 'eval_every': 2000, 'gpu_id': 0, 'hidden_size': 128, 'isTrain': True, 'is_continue': False, 'iters': 50000, 'lambda_align': 0.5, 'lambda_kld': 0.001, 'lambda_trajec': 0.8, 'lie_enforce': False, 'motion_length': 60, 'name': 'act2motion', 'no_trajectory': False, 'plot_every': 50, 'posterior_hidden_layers': 1, 'print_every': 20, 'prior_hidden_layers': 1, 'save_every': 2000, 'save_latest': 50, 'skip_prob': 0, 'tf_ratio': 0.6, 'time_counter': True, 'use_geo_loss': False, 'use_lie': True}\n"
     ]
    }
   ],
   "source": [
    "opt = params.TrainingConfig()\n",
    "print(vars(opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:\" + str(opt.gpu_id) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joints_num = 0\n",
    "input_size = 72\n",
    "data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames 90099, videos 1191, action types 12\n"
     ]
    }
   ],
   "source": [
    "if opt.dataset_type == \"humanact12\":\n",
    "    input_size = 72\n",
    "    joints_num = 24\n",
    "    raw_offsets = paramUtil.humanact12_raw_offsets\n",
    "    kinematic_chain = paramUtil.humanact12_kinematic_chain\n",
    "    data = MotionFolderDatasetHumanAct12(data_path, opt, lie_enforce=opt.lie_enforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 72)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.dim_category = len(data.labels)\n",
    "# arbitrary_len won't limit motion length, but the batch size has to be 1\n",
    "if opt.arbitrary_len:\n",
    "    opt.batch_size = 1\n",
    "    motion_loader = torch.utils.data.DataLoader(data, batch_size=opt.batch_size, drop_last=True, num_workers=1, shuffle=True)\n",
    "else:\n",
    "    motion_dataset = MotionDataset(data, opt)\n",
    "    motion_loader =  torch.utils.data.DataLoader(motion_dataset, batch_size=opt.batch_size, drop_last=True, num_workers=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(motion_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.pose_dim = input_size\n",
    "\n",
    "if opt.time_counter:\n",
    "    opt.input_size = input_size + opt.dim_category + 1\n",
    "else:\n",
    "    opt.input_size = input_size + opt.dim_category\n",
    "\n",
    "opt.output_size = input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9dd6c1257d2836ac92e05bdfd7930286da699a1b9b35da97ed5485e10e2b636d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('Questionnaire': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
